# Дипломный проект DevOps-инженера

## Общая информация

> ФИО: **Асадбеков Асадбеков Давлятбекович**\
> Программа обучения: **DevOps-инженер (Netology)**\
> Группа: **FOPS-29**\
> Дата сдачи: **Декабрь 2025**\
> Статус проекта: **Завершён и готов к защите**\
> *Проект выполнен в рамках учебного дипломного задания и не предназначен для промышленной эксплуатации без дополнительной доработки.*

---

## Содержание

- [Тема дипломной работы](#тема-дипломной-работы)
- [Технический стек и методология работы](#технический-стек-и-методология-работы)
- [Детализированная архитектура решения](#детализированная-архитектура-решения)
- [Реализованные функции и возможности](#реализованные-функции-и-возможности)
- [Репозитории проекта](#репозитории-проекта)
- [Детали реализации CI/CD](#детали-реализации-cicd)
- [Мониторинг и observability](#мониторинг-и-observability)
- [Инструкция по развертыванию](#инструкция-по-развертыванию)
- [Решённые технические вызовы](#решённые-технические-вызовы)
- [Соответствие требованиям дипломного проекта](#соответствие-требованиям-дипломного-проекта)
- [Проверка работоспособности](#проверка-работоспособности)
- [Итоги и выводы](#итоги-и-выводы)
- [Ссылки](#Ссылки-на-репозитории)
- [Лицензия](#лицензия)

---

## Тема дипломной работы

**Разработка и внедрение полного DevOps-пайплайна для веб-приложения с использованием облачной инфраструктуры Yandex Cloud**

Проект представляет собой комплексное решение, охватывающее все этапы современного DevOps-процесса: от проектирования инфраструктуры и настройки Kubernetes-кластера до реализации автоматизированного CI/CD-пайплайна и системы мониторинга. Работа демонстрирует практическое применение принципов `Infrastructure as Code`, `GitOps` и автоматизации развертывания.

---

## Технический стек и методология работы

### Используемые технологии и инструменты

#### Облачная платформа

- **Yandex Cloud** (регион `ru-central1`)
  - Compute Cloud (виртуальные машины)
  - Virtual Private Cloud (VPC)
  - Container Registry

#### Infrastructure as Code

- **Terraform** v1.5+ (основное средство описания инфраструктуры)
- **Terraform Modules** (модульная архитектура конфигураций)

#### Оркестрация контейнеров

- **Kubernetes** v1.28 (self-hosted, развёрнут с помощью `kubeadm`)
- **Flannel** (CNI-плагин для сетевого взаимодействия подов)

#### Сетевой доступ

- **ingress-nginx** (Ingress-контроллер для управления входящим трафиком)
- **NodePort сервисы** (для тестового доступа к приложению)

#### Мониторинг и observability

- **kube-prometheus-stack** (Prometheus Operator)
- **Prometheus** (сбор и хранение метрик)
- **Grafana** (визуализация метрик и дашборды)
- **Alertmanager** (управление оповещениями)

#### CI/CD и автоматизация

- **GitHub Actions** (пайплайны сборки и деплоя)
- **Self-hosted GitHub Actions Runner** (выполнение jobs на master-ноде)
- **Yandex Container Registry** (хранение Docker-образов)

#### Разработка и управление

- **Visual Studio Code** (основная среда разработки)
- **Git** (система контроля версий)
- **SSH-туннелирование** (безопасный доступ к ресурсам)

### Методология и процесс работы

Работа над проектом велась с использованием следующей методологии:

1. **Локальная среда разработки**:
   - Основная работа выполнялась в Visual Studio Code с подключением к Linux-серверу через `SSH`
   - Использование SSH-туннелей для безопасного доступа к Kubernetes API (порт 6443) и другим сервисам
   - Локальные файлы синхронизировались с сервером через `scp`/`rsync`

2. **Подход Infrastructure as Code**:
   - Все компоненты инфраструктуры описаны декларативно в `Terraform`
   - Конфигурации `Kubernetes` хранятся в виде манифестов `YAML`
   - Версионирование всех изменений в `Git`

3. **Поэтапная реализация**:
   - Этап 1: Подготовка облачной инфраструктуры (`Terraform`)
   - Этап 2: Развертывание и настройка `Kubernetes`-кластера
   - Этап 3: Установка системных компонентов (`ingress`, мониторинг)
   - Этап 4: Разработка приложения и `CI/CD`-пайплайна
   - Этап 5: Тестирование и документирование

4. **Принцип минимальных привилегий**:
   - `Service Accounts` с точно определёнными правам 
   - RBAC-правила для `CI/CD runner`
   - Изоляция ресурсов по namespace

---

## Детализированная архитектура решения

### Физическая инфраструктура

```text
Yandex Cloud (ru-central1)
├── Virtual Private Cloud
│   ├── Подсеть в zone-a (192.168.10.0/24)
│   ├── Подсеть в zone-b (192.168.20.0/24)
│   └── Подсеть в zone-c (192.168.30.0/24)
│
├── Виртуальные машины Compute Cloud
│   ├── k8s-master (control-plane)
│   │   ├── 2 vCPU, 4 GB RAM
│   │   ├── Ubuntu 22.04 LTS
│   │   └── Внутренний IP: 192.168.10.32
│   │
│   ├── k8s-worker-1 (worker node)
│   │   ├── 2 vCPU, 4 GB RAM
│   │   ├── Ubuntu 22.04 LTS
│   │   └── Внутренний IP: 192.168.10.33
│   │
│   └── k8s-worker-2 (worker node)
│       ├── 2 vCPU, 4 GB RAM
│       ├── Ubuntu 22.04 LTS
│       └── Внутренний IP: 192.168.20.15
│
└── Container Registry
    └── Хранилище Docker-образов приложения
```

### Логическая архитектура `Kubernetes`	

```text
Kubernetes Cluster (v1.28)
├── Namespace: kube-system
│   └── Системные компоненты Kubernetes
│
├── Namespace: ingress-nginx
│   └── Ingress Controller (Deployment + Service)
│
├── Namespace: monitoring
│   ├── Prometheus Stack (Prometheus, Grafana, Alertmanager)
│   ├── kube-state-metrics
│   └── node-exporter (DaemonSet)
│
├── Namespace: cicd
│   ├── Тестовое приложение (Deployment, 2 реплики)
│   ├── Service (NodePort, порт 30080)
│   ├── Ingress (правила маршрутизации)
│   └── Service Account для GitHub Actions
│
└── Namespace: default
    └── Вспомогательные ресурсы
```
### Сетевые потоки данных

```text
Внешние пользователи
        │
        │  HTTP / HTTPS
        ▼
┌───────────────────────────────┐
│   Ingress Controller          │
│   (ingress-nginx)             │
└───────────────────────────────┘
        │
        │  Внутренний трафик (ClusterIP)
        ▼
┌───────────────────────────────┐
│   Service                     │
│   devops-diplom-app-service   │
└───────────────────────────────┘
        │
        │  Load Balancing
        ▼
┌───────────────────┐   ┌───────────────────┐
│ Pod 1             │   │ Pod 2             │
│ devops-diplom-app │   │ devops-diplom-app │
└───────────────────┘   └───────────────────┘
```

## Реализованные функции и возможности

### 1. Автоматизированная инфраструктура

- **Terraform-конфигурации** для полного воссоздания инфраструктуры  
- **Модульная структура** (vpc, compute, registry и др.)  
- **Автоматическое назначение IP-адресов** и настройка сетей  
- **Гибкое описание ресурсов**: использование `for_each` и структурированных переменных (`map(object)`) для масштабируемости Terraform-конфигураций

### 2. Высокодоступный Kubernetes-кластер

- **Self-hosted кластер** с ручной настройкой через `kubeadm`  
- **Разделение ролей** (control-plane, worker nodes)  
- **Размещение в разных зонах доступности** для отказоустойчивости  
- **Настройка CNI** (Flannel) для сетевого взаимодействия подов  

### 3. Полноценный CI/CD-пайплайн

- **Автоматическая сборка** Docker-образов при каждом push в `main`
- **Версионный деплой**: деплой в Kubernetes выполняется **только при push тега версии** (`vX.Y.Z`)
  - Исключены лишние деплои и обновления на `latest`
  - Образы тегируются по версии и SHA коммита
- **Разделение стадий build и deploy**, исключающее случайный деплой незавершённого кода
- **Тегирование образов** по SHA коммита и версиям (напр., `v1.0.0`)  
- **Безопасный деплой** с использованием Service Account 
- **Rolling updates** без downtime (`kubectl set image`)
- **Self-hosted runner** для выполнения jobs непосредственно в кластере

### 4. Особенность реализации приложения

- Пользовательский HTML-контент приложения вынесен в `ConfigMap`, который монтируется в Pod. Это позволяет изменять содержимое страницы **без пересборки Docker-образа и повторного деплоя приложения**, что соответствует best practices Kubernetes и снижает время обновления.

### 5. Комплексный мониторинг

- **Автоматическое обнаружение** целей мониторинга (`ServiceMonitor`)
- **Предустановленные дашборды** `Grafana` для `Kubernetes`
- **Мониторинг на всех уровнях**: инфраструктура, кластер, приложения
- **Готовность к настройке алертинга** через `Alertmanager`

### 6. Безопасность и управление доступом

- **RBAC-настройки** для `CI/CD runner`  
- **Secret management** через `Kubernetes Secrets`  
- **Изоляция окружений** через `namespaces`  
- **Доступ к registry** через `imagePullSecrets`

## Репозитории проекта

| Репозиторий | Назначение | Ключевые компоненты |
|-------------|-----------|-------------------|
| **[Infrastructure](https://github.com/asad-bekov/devops-diplom-infra)** | Terraform-конфигурации инфраструктуры | • Модули VPC, Compute, Registry<br>• Bootstrap-конфигурации<br>• Модульная структура Terraform-конфигураций |
| **[Kubernetes](https://github.com/asad-bekov/devops-diplom-k8s)** | Манифесты и конфигурации Kubernetes | • Ingress-nginx контроллер<br>• kube-prometheus-stack<br>• Системные компоненты |
| **[Application](https://github.com/asad-bekov/devops-diplom-app)** | Приложение и CI/CD-пайплайн | • Dockerfile и исходный код<br>• GitHub Actions workflow<br>• Kubernetes манифесты приложения |

Все репозитории связаны между собой и представляют единое решение. Изменения в инфраструктуре (Terraform) приводят к обновлению кластера, что в свою очередь влияет на работу приложений, развернутых через CI/CD.

---

## Детали реализации CI/CD

### Особенности реализации

- **Двухэтапный процесс**: отдельные `jobs` для сборки и деплоя  
- **Self-hosted runner**: выполнение `deploy job` на `master`-ноде кластера  
- **Безопасные секреты**: все `sensitive` данные в `GitHub Secrets`  
- **Идемпотентность**: `kubectl apply` обеспечивает повторяемость деплоя  
- **Zero-downtime updates**: стратегия `rolling update` с проверкой готовности
- **Контроль точки деплоя**: job деплоя запускается только при сборке тега версии, что соответствует production-практикам
- **Отсутствие лишних деплоев**: деплой выполняется сразу нужной версии образа без промежуточного обновления на `latest`

---

## Мониторинг и observability

### Настроенные компоненты

- Prometheus Server с автоматическим обнаружением целей  
- Grafana с предустановленными дашбордами:
  - `Kubernetes / Compute Resources / Cluster`
  - `Kubernetes / API Server`
  - `Kubernetes / Compute Resources / Namespace (Pods)`
  - `Kubernetes / Kubelet`
- `Alertmanager` (готов к настройке правил оповещения)  
- `kube-state-metrics` для метрик состояния объектов `Kubernetes`  
- `node-exporter` для метрик уровня операционной системы

### Ключевые метрики

- **Инфраструктура**: загрузка CPU, использование памяти, дисковое пространство  
- **Kubernetes**: состояние нод, количество подов, использование ресурсов  
- **Приложение**: доступность (uptime), время ответа, ошибки

### Доступ к мониторингу

- **Grafana**: `http://<worker-node-ip>:30081` (admin/пароль из Secret)  
- **Prometheus**: `kubectl port-forward svc/prometheus-k8s 9090:9090`  
- **Alertmanager**: `kubectl port-forward svc/alertmanager 9093:9093`

---

## Инструкция по развертыванию

### Предварительные требования

- Аккаунт в Yandex Cloud с достаточными квотами  
- Установленные локально: `terraform`, `kubectl`, `git`  
- Доступ к GitHub для создания репозиториев и настройки Secrets  

### Шаги развертывания

```bash
# 1. Клонирование репозиториев
git clone https://github.com/asad-bekov/devops-diplom
git clone https://github.com/asad-bekov/devops-diplom-infra
git clone https://github.com/asad-bekov/devops-diplom-k8s
git clone https://github.com/asad-bekov/devops-diplom-app

# 2. Развертывание инфраструктуры
cd devops-diplom-infra/prepare
terraform init && terraform apply
cd ../production
terraform init && terraform apply

# 3. Запуск CI/CD пайплайна
# Произвести push в main ветку devops-diplom-app
```

## Решённые технические вызовы

### Проблема 1: Доступ к Kubernetes API из GitHub Actions  

**Проблема**: Ошибка подключения к `localhost:8080` при выполнении `kubectl` команд в workflow.  
**Решение**:  
- Создание `Service Account` с необходимыми правами  
- Генерация `kubeconfig` с токеном `SA`  
- Использование этого `kubeconfig` в `GitHub Actions` через `Secrets`  
- Настройка `RBAC` правил для ограничения доступа к namespace `cicd`

### Проблема 2: ImagePullBackOff при деплое приложения  

**Проблема**: Поды не могут загрузить образ из приватного `Container Registry`.  
**Решение**:  
- Создание Secret типа `docker-registry` с учётными данными Yandex Cloud  
- Добавление `imagePullSecrets` в спецификацию Deployment  
- Проверка правильности формирования `dockerconfigjson`

### Проблема 3: Настройка self-hosted GitHub Actions runner  

**Проблема**: Обеспечение безопасного и надёжного выполнения `jobs` на master-ноде.  
**Решение**:  
- Установка и регистрация runner на master-ноде  
- Настройка `labels` для выбора конкретного `runner`  
- Ограничение прав `runner` через отдельный `Service Account`

### Проблема 4: Управление state Terraform в команде  

- Terraform-конфигурации проекта являются воспроизводимыми и позволяют развернуть инфраструктуру с нуля. В рамках дипломного проекта используется локальный запуск Terraform без обязательного `remote backend`, что соответствует требованиям задания.

---

## Соответствие требованиям дипломного проекта

| Требование                                 | Реализация                                    | Статус        |
|--------------------------------------------|-----------------------------------------------|----------------|
| Репозиторий с Terraform-конфигурациями     | `devops-diplom-infra`                         | Выполнено    |
| Репозиторий с Kubernetes-манифестами       | `devops-diplom-k8s`                           | Выполнено    |
| Репозиторий с Dockerfile и CI/CD           | `devops-diplom-app`                           | Выполнено    |
| Self-hosted Kubernetes кластер             | 3 ноды (1 master, 2 worker)                   | Выполнено    |
| Работоспособный CI/CD pipeline             | GitHub Actions с `self-hosted runner`           | Выполнено    |
| Система мониторинга                        | `Prometheus` + `Grafana в namespace monitoring` | Выполнено    |
| Скриншоты всех этапов                      | В каждом репозитории в папке `screenshots/`   | Выполнено    |
| Документация                               | Подробные `README.md` в каждом репозитории    | Выполнено    |
| Воспроизводимость                          | Terraform-конфигурации                        | Выполнено    |

---

## Проверка работоспособности

### Команды для проверки

```bash
# 1. Проверка инфраструктуры
kubectl get nodes -o wide
# Ожидаемый результат: 3 ноды в состоянии Ready

# 2. Проверка системных компонентов
kubectl get pods -n ingress-nginx
kubectl get pods -n monitoring
# Ожидаемый результат: все поды в состоянии Running

# 3. Проверка приложения
kubectl get deployment,pods,svc,ingress -n cicd
# Ожидаемый результат: Deployment с 2 репликами, Service и Ingress

# 4. Проверка доступности приложения
curl http://<worker-node-ip>:30080
# Ожидаемый результат: HTML страница приложения

# 5. Проверка мониторинга
kubectl get svc -n monitoring | grep grafana
# Ожидаемый результат: сервис Grafana доступен на порту 30081
```

### Скриншоты подтверждения

Все этапы работы задокументированы скриншотами, расположенными в соответствующих репозиториях:

- Настройка и состояние инфраструктуры  
- Работа Kubernetes-кластера  
- Выполнение CI/CD пайплайна  
- Интерфейсы мониторинга (Grafana дашборды)  
- Доступность приложения  

---

## Итоги и выводы

В результате выполнения дипломного проекта была разработана и успешно внедрена комплексная DevOps-платформа, охватывающая все ключевые аспекты современной практики разработки и эксплуатации программного обеспечения.

### Ключевые достижения:

- Полная автоматизация от кода до production-окружения  
- Воспроизводимость инфраструктуры через `Infrastructure as Code`  
- Надёжный и отказоустойчивый `Kubernetes`-кластер  
- Эффективный CI/CD-пайплайн с `zero-downtime` деплоями  
- Комплексная система мониторинга для `observability`  
- Безопасность и управление доступом на всех уровнях  

### Практическая ценность:

Проект демонстрирует готовность к решению реальных задач DevOps-инженера, включая проектирование облачной инфраструктуры, настройку Kubernetes, автоматизацию процессов и обеспечение надёжности систем. Все решения основаны на современных best practices и могут быть использованы как основа для дальнейшего развития production-проектов.

### Перспективы развития:

- Расширение мониторинга кастомными метриками приложения  
- Внедрение GitOps-подхода через ArgoCD  
- Настройка многоступенчатых окружений (`dev`/`staging`/`prod`)  
- Внедрение security scanning в CI/CD pipeline  
- Автоматизация бэкапов и disaster recovery  

---

### Ссылки на репозитории:

- [Инфраструктура](https://github.com/asad-bekov/devops-diplom-infra)  
- [Kubernetes конфигурации](https://github.com/asad-bekov/devops-diplom-k8s)  
- [Приложение и CI/CD](https://github.com/asad-bekov/devops-diplom-app)  

---

## Лицензия

Проект распространяется под лицензией **MIT**. Подробная информация доступна в файлах `LICENSE` каждого репозитория.
